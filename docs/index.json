
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["orazio-gallo"],"categories":null,"content":"I am a Principal Research Scientist and Senior Research Manager at NVIDIA Research, which I joined in 2011. I am interested in computational photography and low-level computer vision particularly for autonomous navigation and robotics applications.\nI am a senior associate editor of the IEEE Transactions on Computational Imaging. From 2018 to 2022 I was a member of the IEEE Computational Imaging Technical Committee, and from 2015 to 2017 I was an associate editor for Signal processing: Image Communication.\nWant to join?\nIf you have similar research interests including, but not limited to anything 3D (e.g., monocular/multi-view depth estimation, SLAM, SfM, etc.), anything optical/scene flow, anything novel view synthesis, please reach out. We have internships (for PhD students) and full-time positions. Shoot me an email!\n","date":1727740800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1727740800,"objectID":"50c607b005f6e06669e07c72ff9edbc7","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Principal Research Scientist and Senior Research Manager at NVIDIA Research, which I joined in 2011. I am interested in computational photography and low-level computer vision particularly for autonomous navigation and robotics applications.","tags":null,"title":"Orazio Gallo","type":"authors"},{"authors":null,"categories":null,"content":"","date":1549324800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"Orazio Gallo","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemyâ€™s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"d94758b76a8633be1c27f334552036fd","permalink":"https://oraziogallo.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Ekta Prashnani","Koki Nagano","Shalini De Mello","David Luebke","Orazio Gallo"],"categories":null,"content":"","date":1727740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727740800,"objectID":"0f0e4bde0bbeaf8724e0ca4cd2d1c89d","permalink":"https://oraziogallo.github.io/publication/prashnani2023avatar/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/prashnani2023avatar/","section":"publication","summary":"","tags":null,"title":"Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos","type":"publication"},{"authors":["Daniel Lichy","Hang Su","Abhishek Badki","Jan Kautz","Orazio Gallo"],"categories":null,"content":"","date":1709251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709251200,"objectID":"b0e8c70c0afd9f8128e5c1346c145f44","permalink":"https://oraziogallo.github.io/publication/lichy2024fovagnostic/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/lichy2024fovagnostic/","section":"publication","summary":"","tags":null,"title":"FoVA-Depth: Field-of-View Agnostic Depth Estimation for Cross-Dataset Generalization","type":"publication"},{"authors":["Jiashun Wang","Xueting Li","Sifei Liu","Shalini De Mello","Orazio Gallo","Xiaolong Wang","Jan Kautz"],"categories":null,"content":"","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"82ba2429d9a769520f39c68379061c3f","permalink":"https://oraziogallo.github.io/publication/wang2023posetransfer/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/wang2023posetransfer/","section":"publication","summary":"","tags":null,"title":"Zero-shot Pose Transfer for Unrigged Stylized 3D Characters","type":"publication"},{"authors":["Eric Chan","Connor Z. Lin","Matthew A. Chan","Koki Nagano","Boxiao Pan","Shalini De Mello","Orazio Gallo","Leonidas J. Guibas","Jonathan Tremblay","Sameh Khamis","Tero Karras","Gordon Wetzstein"],"categories":null,"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"529e3f701b9ad0b0f4938e1ae382a61c","permalink":"https://oraziogallo.github.io/publication/chan2021eg3d/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/chan2021eg3d/","section":"publication","summary":"","tags":null,"title":"Efficient Geometry-aware 3D Generative Adversarial Networks","type":"publication"},{"authors":["Atsuhiro Noguchi","Umar Iqbal","Jonathan Tremblay","Tatsuya Harada","Orazio Gallo"],"categories":null,"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"0a69a27c73f6b94e6b36ec34ad2d1f78","permalink":"https://oraziogallo.github.io/publication/noguchi2022watch/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/noguchi2022watch/","section":"publication","summary":"","tags":null,"title":"Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects","type":"publication"},{"authors":null,"categories":null,"content":" Outstanding Editorial Board Member Award for my service to the IEEE Trans. on Comp. Imaging. Outstanding Reviewer Award for CVPR 2017, CVPR 2018, CVPR 2021, ICCV 2021, and BMVC 2021. ","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637539200,"objectID":"5d66cf956cd8d432565d6e9dcabe8ddd","permalink":"https://oraziogallo.github.io/news/2023-10_awards/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/news/2023-10_awards/","section":"news","summary":"Outstanding Editorial Board Member Award for my service to the IEEE Trans. on Comp. Imaging. Outstanding Reviewer Award for CVPR 2017, CVPR 2018, CVPR 2021, ICCV 2021, and BMVC 2021.","tags":null,"title":"","type":"news"},{"authors":["Ekta Prashnani","Orazio Gallo","Joohwan Kim","Josef B. Spjut","Pradeep Sen","Iuri Frosio"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"b9718bf864f543b04752721f9ec12677","permalink":"https://oraziogallo.github.io/publication/prashnani2021noiseawarevs/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/prashnani2021noiseawarevs/","section":"publication","summary":"","tags":null,"title":"Noise-Aware Video Saliency Prediction","type":"publication"},{"authors":["Abhishek Badki","Orazio Gallo","Jan Kautz","Pradeep Sen"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"b4db2555eb278e589e0a5d2b66769cfe","permalink":"https://oraziogallo.github.io/publication/badki2021bittc/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/badki2021bittc/","section":"publication","summary":"","tags":null,"title":"Binary TTC: A Temporal Geofence for Autonomous Navigation","type":"publication"},{"authors":["Chaoyang Wang","Ben Eckart","Simon Lucey","Orazio Gallo"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"affa63b42946502568387cf6e24fbe47","permalink":"https://oraziogallo.github.io/publication/wang2021neuraltf/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/wang2021neuraltf/","section":"publication","summary":"","tags":null,"title":"Neural Trajectory Fields for Dynamic Novel View Synthesis","type":"publication"},{"authors":["Tewodros Amberbir Habtegebrial","Varun Jampani","Orazio Gallo","Didier Stricker"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"adefce131a741b8e05dcb09816b87301","permalink":"https://oraziogallo.github.io/publication/habtegebrial2020generativevs/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/habtegebrial2020generativevs/","section":"publication","summary":"","tags":null,"title":"Generative View Synthesis: From Single-view Semantics to Novel-view Images","type":"publication"},{"authors":["Abhishek Badki","Alejandro Troccoli","Kihwan Kim","Jan Kautz","Pradeep Sen","Orazio Gallo"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"8846c2407f47087461593b8ae70bccc1","permalink":"https://oraziogallo.github.io/publication/badki2020bi3d/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/badki2020bi3d/","section":"publication","summary":"","tags":null,"title":"Bi3D: Stereo Depth Estimation via Binary Classifications","type":"publication"},{"authors":["Abhishek Badki","Orazio Gallo","Jan Kautz","Pradeep Sen"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"3dfecf60e93ef5d61ce3dfb5cdd90d6e","permalink":"https://oraziogallo.github.io/publication/badki2020meshlets/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/badki2020meshlets/","section":"publication","summary":"","tags":null,"title":"Meshlet Priors for 3D Mesh Reconstruction","type":"publication"},{"authors":["Jae Shin Yoon","Kihwan Kim","Orazio Gallo","Hyun Soo Park","Jan Kautz"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"f565ba0428fdab9607dc22696974b5d8","permalink":"https://oraziogallo.github.io/publication/yoon2020novelvs/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/yoon2020novelvs/","section":"publication","summary":"","tags":null,"title":"Novel View Synthesis of Dynamic Scenes With Globally Coherent Depths From a Monocular Camera","type":"publication"},{"authors":["Inchang Choi","Orazio Gallo","Alejandro Troccoli","Min H. Kim","Jan Kautz"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"b902b38e4f56b37fd405d29d9b9e30d9","permalink":"https://oraziogallo.github.io/publication/choi2019evs/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/choi2019evs/","section":"publication","summary":"","tags":null,"title":"Extreme View Synthesis","type":"publication"},{"authors":["Wei-Sheng Lai","Orazio Gallo","Jinwei Gu","Deqing Sun","Ming-Hsuan Yang","Jan Kautz"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"8ac524c401e41f793547ec780ca327df","permalink":"https://oraziogallo.github.io/publication/lai2019stitching/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/lai2019stitching/","section":"publication","summary":"","tags":null,"title":"Video Stitching for Linear Camera Arrays","type":"publication"},{"authors":["Hang Su","Varun Jampani","Deqing Sun","Orazio Gallo","Erik-Learned Miller","Jan Kautz"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"96d6b43be2af7cc7392c110fbd87b809","permalink":"https://oraziogallo.github.io/publication/su2019pac/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/su2019pac/","section":"publication","summary":"","tags":null,"title":"Pixel-Adaptive Convolutional Neural Networks","type":"publication"},{"authors":["Orazio Gallo"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your postâ€™s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://oraziogallo.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Letâ€™s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://oraziogallo.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Zhile Ren","Orazio Gallo","Deqing Sun","Ming-Hsuan Yang","Erik B. Sudderth","Jan Kautz"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7c4e560ee6c1ee1f14b3a97c2574d393","permalink":"https://oraziogallo.github.io/publication/ren2019fusion/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/ren2019fusion/","section":"publication","summary":"","tags":null,"title":"A Fusion Approach for Multi-Frame Optical Flow Estimation","type":"publication"},{"authors":["Patrick Wieschollek","Orazio Gallo","Jinwei Gu","Jan Kautz"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"9670f7b6a1581fefcc4177ed88493ea4","permalink":"https://oraziogallo.github.io/publication/wieschollek2018reflections/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/wieschollek2018reflections/","section":"publication","summary":"","tags":null,"title":"Separating Reflection and Transmission Images in the Wild","type":"publication"},{"authors":["Qi Guo","Iuri Frosio","Orazio Gallo","Todd Zickler","Jan Kautz"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"1ce836847c609831ecb1799dc8dd8033","permalink":"https://oraziogallo.github.io/publication/guo2018/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/guo2018/","section":"publication","summary":"","tags":null,"title":"Tackling 3D ToF Artifacts Through Learning and the FLAT Dataset","type":"publication"},{"authors":["Huaijin Chen","Jinwei Gu","Orazio Gallo","Ming-Yu Liu","Ashok Veeraraghavan","Jan Kautz"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"cfac6ea8e298a21b80c3c5967d02de1c","permalink":"https://oraziogallo.github.io/publication/chen2018deblur/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/chen2018deblur/","section":"publication","summary":"","tags":null,"title":"Reblur2Deblur: Deblurring Videos via Self-Supervised Learning","type":"publication"},{"authors":["Abhishek Badki","Orazio Gallo","Jan Kautz","Pradeep Sen"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"d60fae7d4afcf091a063824dc4104114","permalink":"https://oraziogallo.github.io/publication/badki2017compzoom/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/badki2017compzoom/","section":"publication","summary":"","tags":null,"title":"Computational Zoom: A Framework for Post-Capture Image Composition","type":"publication"},{"authors":["Suren Jayasuriya","Orazio Gallo","Jinwei Gu","Timo Aila","Jan Kautz"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"8791778971b83d89cfd79b4f066e90ff","permalink":"https://oraziogallo.github.io/publication/jayasuriya2017gradient/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/jayasuriya2017gradient/","section":"publication","summary":"","tags":null,"title":"Reconstructing Intensity Images from Binary Spatial Gradient Cameras","type":"publication"},{"authors":["Hang Zhao","Orazio Gallo","Iuri Frosio","Jan Kautz"],"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"defd5ceb6513ba3c51e6695ba56ef880","permalink":"https://oraziogallo.github.io/publication/zhao2017losses/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/zhao2017losses/","section":"publication","summary":"","tags":null,"title":"Loss Functions for Image Restoration with Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"News 22\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"6293397915c247b3efe576c9ded587ea","permalink":"https://oraziogallo.github.io/news/2023-11_accepted/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/news/2023-11_accepted/","section":"news","summary":"News 22","tags":null,"title":"","type":"news"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://oraziogallo.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Orazio Gallo","Alejandro Troccoli","Jun Hu","Kari Pulli","Jan Kautz"],"categories":null,"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"722078cc2e5678aac4d8e1b0d968429d","permalink":"https://oraziogallo.github.io/publication/gallo2015fastnrr/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/gallo2015fastnrr/","section":"publication","summary":"","tags":null,"title":"Locally Non-Rigid Registration for Mobile HDR Photography","type":"publication"},{"authors":["Orazio Gallo","Iuri Frosio","Leonardo Gasparini","Kari Pulli","Massimo Gottardi"],"categories":null,"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"1894cc6bc628e0321db0e58aa36babab","permalink":"https://oraziogallo.github.io/publication/gallo2015binary/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/gallo2015binary/","section":"publication","summary":"","tags":null,"title":"Retrieving Gray-Level Information from a Binary Sensor and its Application to Gesture Detection","type":"publication"},{"authors":["Felix Heide","Michael Steinberger","Yun-Ta Tsai","Mushfiqur (Nasa) Rouf","Dawid Pajak","Dikpal Reddy","Orazio Gallo","Jing Liu","Wolfgang Heidrich","Karen Egiazarian","Jan Kautz","Kari Pulli"],"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"e2a5538f311ee0daaaf36dfe800be8fb","permalink":"https://oraziogallo.github.io/publication/heide2014flexisp/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/heide2014flexisp/","section":"publication","summary":"","tags":null,"title":"FlexISP: A Flexible Camera Image Processing Framework","type":"publication"},{"authors":["David E. Jacobs","Orazio Gallo","Emily Cooper","Kari Pulli","Marc Levoy"],"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"36192bb7d737b3329221253d60aa7b52","permalink":"https://oraziogallo.github.io/publication/jacobs2015gaze/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/jacobs2015gaze/","section":"publication","summary":"","tags":null,"title":"Simulating the Visual Experience of Very Bright and Very Dark Scenes","type":"publication"},{"authors":["David E. Jacobs","Orazio Gallo","Kari Pulli"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"d28f87fbe0ec8dc5f78ce32350311183","permalink":"https://oraziogallo.github.io/publication/jacobs2014dynamic/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/jacobs2014dynamic/","section":"publication","summary":"","tags":null,"title":"Dynamic Image Stacks","type":"publication"},{"authors":["Jun Hu","Orazio Gallo","Kari Pulli","Xiaobai Sun"],"categories":null,"content":"","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"29a6453cffca6ba6e63ddbc8bb2c1193","permalink":"https://oraziogallo.github.io/publication/hu2013hdr/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/hu2013hdr/","section":"publication","summary":"","tags":null,"title":"HDR Deghosting: How to deal with Saturation?","type":"publication"},{"authors":["Jun Hu","Orazio Gallo","Kari Pulli"],"categories":null,"content":"","date":1351728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351728000,"objectID":"212657c94b1897eebb00d60f52763c9a","permalink":"https://oraziogallo.github.io/publication/hu2012stacks/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/hu2012stacks/","section":"publication","summary":"","tags":null,"title":"Exposure Stacks for Live Scenes with Hand-held Cameras","type":"publication"},{"authors":["Orazio Gallo","Marius Tico","Roberto Manduchi","Natasha Gelfand","Kari Pulli"],"categories":null,"content":"","date":1335830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1335830400,"objectID":"5d7936339bdf71edbdaa612bcf4e040f","permalink":"https://oraziogallo.github.io/publication/gallo2012metering/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/gallo2012metering/","section":"publication","summary":"","tags":null,"title":"Metering for Exposure Stacks","type":"publication"},{"authors":["Orazio Gallo","Roberto Manduchi","Abbas Rafii"],"categories":null,"content":"","date":1296518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296518400,"objectID":"87d093d34e59fc509567eecae9ae1646","permalink":"https://oraziogallo.github.io/publication/gallo2012ccransac2/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/gallo2012ccransac2/","section":"publication","summary":"","tags":null,"title":"CC-RANSAC: Fitting Planes in the Presence of Multiple Surfaces in Range Data","type":"publication"},{"authors":["Orazio Gallo","Roberto Manduchi"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"baacb22d4496835b9a07a4b698509768","permalink":"https://oraziogallo.github.io/publication/gallo2011barcodes/","publishdate":"1900-01-01T00:00:00Z","relpermalink":"/publication/gallo2011barcodes/","section":"publication","summary":"","tags":null,"title":"Reading 1D Barcodes with Mobile Phones Using Deformable Templates","type":"publication"}]